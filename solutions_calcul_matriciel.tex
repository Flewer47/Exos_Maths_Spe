\section{Calcul matriciel}

\begin{proof}
    Soit $(k,m)\in\left(\N^{*}\right)^{2}$, on a 
    \begin{align}
        \left[M\overline{M}\right]_{k,m}
        &=\sum_{j=1}^{n}\omega^{(k-1)(j-1)}\overline{\omega}^{(j-1)(m-1)}\\
        &=\sum_{j=0}^{n-1}\left[\omega^{k-1}\overline{\omega}^{m-1}\right]^{j}\\
        &=\sum_{j=0}^{n-1}\left[\omega^{k-m}\right]^{j}
    \end{align}

    Or $\omega^{k-m}=1$ si et seulement si $n\mid k-m$ si et seulement si $k=m$ car $\left\lvert k-m\right\rvert\in\llbracket0,n-1\rrbracket$. Si $k=m$, on a $[M\overline{M}]_{k,m}=n$ et si $k\neq m$, on a 
    \begin{equation}
        [M\overline{M}]_{k,m}=\frac{1-\left(\omega^{k-m}\right)^{n}}{1-\omega^{k-m}}=0
    \end{equation}
    Donc $M\overline{M}=nI_{n}$. Ainsi, $M\in GL_{n}(\C)$ et 
    \begin{equation}
        \boxed{M^{-1}=\frac{1}{n}\overline{M}}
    \end{equation}

    On a $\det(M\overline{M})=\det(M)\det(\overline{M})=n^{n}=\det(M)\overline{\det(M)}=\left\lvert\det(M)\right\rvert^{2}$ donc $\left\lvert\det(M)\right\rvert=n^{\frac{n}{2}}$.

    On calcul $M^{2}$. On a 
    \begin{equation}
        [M^{2}]_{k,m}=\sum_{j=1}^{n}\omega^{(k-1)(j-1)+(j-1)(m-1)}=\sum_{j=0}^{n-1}\left[\omega^{k+m-2}\right]^{j}
    \end{equation}

    On a $k+m-2\in\llbracket0,2n-2\rrbracket$ donc $n\mid k+m-2$ si et seulement si $k+m=n+2$ ou $k+m=2$ si et seulement si $m=n+2-k$ ou $k=m=1$. Donc 
    \begin{equation}
        M^{2}=
        \begin{pmatrix}
            n       & 0         & \dots     & \dots & 0\\
            0       &           &           &       & n\\
            \vdots  &           &           & n     & 0\\
            \vdots  &           & \iddots   &       & \vdots\\
            0       & n         &\dots      &\dots  &0
        \end{pmatrix}
    \end{equation}

    En développant par rapport à la première ligne (ou colonne), on a 
    \begin{equation}
        \det(M^{2})=n^{n}(-1)^{\frac{n(n+1)}{2}}
    \end{equation}

    donc 
    \begin{equation}
        \boxed{\det(M)=
        \left\lbrace
            \begin{array}[]{ll}
                \pm n^{\frac{n}{2}} &\text{si }\frac{n(n+1)}{2}\text{ est pair i.e. }
                \left\lbrace
                \begin{array}[]{l}
                    n\equiv 0[4]\\
                    \text{ou}\\
                    n\equiv 3[4]
                \end{array}
                \right.\\
                \pm \i n^{\frac{n}{2}} &\text{si }\frac{n(n+1)}{2}\text{ est impair i.e. }
                \left\lbrace
                \begin{array}[]{l}
                    n\equiv 1[4]\\
                    \text{ou}\\
                    n\equiv 2[4]
                \end{array}
                \right.\\
            \end{array}
        \right.
        }
    \end{equation}
\end{proof}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Si $A\geqslant0$, soit $X\geqslant0$, on a 
        \begin{equation}
            [AX]_{i}=\sum_{j=1}^{n}a_{i,j}x_{j}\geqslant 0
        \end{equation}
        donc $AX\geqslant0$.

        Réciproquement, soit $j\in\llbracket1,n\rrbracket$, on prend 
        \begin{equation}
            X_{j}=
            \begin{pmatrix}
                0\\
                \vdots\\
                0\\
                1\\
                0\\
                \vdots\\
                0    
            \end{pmatrix}
        \end{equation}
        où le 1 est en $j$-ième position. $X_{j}\geqslant0$ et 
        \begin{equation}
            AX=
            \begin{pmatrix}
                a_{1,j}\\
                \vdots\\
                a_{n,j}
            \end{pmatrix}
            \geqslant0
        \end{equation}
        donc $A\geqslant0$.

        \item Soit $A\in GL_{n}(\R)$ avec $A=(A_{i,j})_{1\leqslant i,j\leqslant n}\geqslant0,A^{-1}=(A^{-1})_{1\leqslant i,j\leqslant n}\geqslant0$. Soit $(i,j)\in\llbracket1,n\rrbracket^{2}$ avec $i\neq j$. On a 
        \begin{equation}
            \sum_{k=1}^{n}A_{i,k}A^{-1}_{k,j}=0
        \end{equation}
        donc pour tout $k\in\llbracket1,n\rrbracket$ on a $A_{i,j}=0$ ou $A^{-1}_{k,j}=0$.

        $i$ étant fixé, comme $A\in Gl_{n}(\R)$, il existe $k_{0}\in\llbracket1,n\rrbracket$ tel que $A_{i,k_{0}}>0$. Alors pour tout $j\in\llbracket1,n\rrbracket\setminus\lbrace i\rbrace$, on a $A^{-1}_{k_{0},j}=0$ et $A^{-1}_{k_{0},i}>0$ (car $A^{-1}$ est inversible). Supposons qu'il existe $k_{1}\neq k_{0}$ tel que $A_{i,k_{1}}>0$. Alors pour tout $j\neq i$, on a $A^{-1}_{k,j}=0$ et $A^{-1}_{k_{1},i}>0$, mais alors les lignes $k_{0}$ et $k_{1}$ sont liées, ce qui est impossible. Donc il existe un unique $k_{i}\in\llbracket1,n\rrbracket$, $A_{i,k_{i}}>0$.

        Comme $A$ est inversible, pour $i\neq i'$, on a $k_{i}\neq k_{i'}$, sinon on aurait deux lignes proportionnelles. Donc \function{\Delta}{\llbracket1,n\rrbracket}{\llbracket1,n\rrbracket}{i}{k_{i}}
        Ainsi il existe une unique permutation $\sigma\in\Sigma_{n}$ telle que pour tout $i\in\llbracket1,n\rrbracket$, $A_{i,\sigma(i)}>0$ et pour tout $j\neq\sigma(i)$, $A_{ij}=0$. Donc 
        \begin{equation}
            \boxed{A=\diag(a_{1},\dots,a_{n})P_{\sigma}}
        \end{equation}
        avec $P_{\sigma}=(\delta_{i,\sigma(j)})_{i,j}$ et $a_{i}>0$.

        Réciproquement, si $A$ est de cette forme, on a $A\geqslant0$ et 
        \begin{equation}
            A^{-1}=P^{-1}_{\sigma}\diag\left(\frac{1}{a_{1}},\dots,\frac{1}{a_{n}}\right)=P_{\sigma^{-1}}\diag\left(\frac{1}{a_{1}},\dots,\frac{1}{a_{n}}\right)
        \end{equation}
        donc $A^{-1}\geqslant0$.
    \end{enumerate}
\end{proof}

\begin{remark}
    Soit 
    \begin{equation}
        A=
        \begin{pmatrix}
            2   & -1    & 0   &\dots &0\\
            -1  & \ddots&\ddots     &\ddots&\vdots\\
            0   & \ddots&\ddots     &\ddots&0\\
            \vdots&\ddots&\ddots&\ddots&-1\\
            0&\dots&0&-1&2
        \end{pmatrix}
    \end{equation}

    Si $AX\geqslant0$, en définissant $x_{0}=x_{n+1}=0$, on a pour tout $k\in\llbracket1,n\rrbracket$,
    \begin{equation}
        -x_{k-1}+2x_{k}-x_{k+1}\geqslant0
    \end{equation}
    Si $x_{i_{0}}=\min(x_{0},\dots,x_{n+1})$, on a 
    \begin{equation}
        2x_{i_{0}}\geqslant x_{i_{0}-1}+x_{i_{0}+1}\geqslant 2x_{i_{0}}
    \end{equation}
    donc $x_{i_{0}-1}=x_{i_{0}+1}=x_{i_{0}}$. De proche en proche, on a $x_{i_{0}}=x_{0}=0$. Donc $X\geqslant0$.

    Si $AX=0$, on a $AX\geqslant0$ et $A(-X)=0$ donc $X\geqslant0$ et $-X\geqslant0$ donc $X=0$ et $A\in GL_{n}(\R)$ et pour tout $Y=AX\geqslant0$, on a $A^{-1}Y=X\geqslant0$ donc $A^{-1}\geqslant0$.
\end{remark}

\begin{proof}
    Soit \function{u}{\R_{n-1}[X]}{\R_{n-1}[X]}{P}{P(X+1)}
    Pour tout $j\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X+1)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}
    \end{equation}
    On note $P_{i}=X^{i-1}$ et $\mathcal{B}=(P_{1},\dots,P_{n})$ la base canonique de $\R_{n-1}[X]$. On note $A=\mat_{\mathcal{B}}(u)$. $u^{-1}\colon P\mapsto P(X-1)$ donc $A$ est inversible et pour tout $k\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X-1)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}(-1)^{j-i-1}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}(-1)^{j-i}
    \end{equation}
    donc 
    \begin{equation}
        \boxed{A^{-1}=\left(\binom{j-1}{i-1}(-1)^{j-i}\right)_{1\leqslant i,j\leqslant n}}
    \end{equation}

    Pour tout $k\in\N$, on a $u^{k}\colon P\mapsto P(X+k)$ et pour tout $j\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X+k)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}k^{j-i-1}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}k^{j-i}
    \end{equation}
    donc 
    \begin{equation}
        \boxed{A^{k}=\left(\binom{j-1}{i-1}k^{j-i}\right)_{1\leqslant i,j\leqslant n}}
    \end{equation}
\end{proof}

\begin{proof}
    \label{sol:4.4}
    \phantom{}
    \begin{enumerate}
        \item Pour $n\in\N^{*}$, on note $H(n)$:`si $\dim(E)=n$ et si $u\in\L(E)$ vérifie $\Tr(u)=0$, alors il existe une base $\mathcal{B}$ de $E$ telle que 
        \begin{equation}
            \mat_{\mathcal{B}}(u)=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
        \end{equation}'

        Pour $n=1$, on a $u=0$ si $\Tr(u)=0$. Pour $n\geqslant1$, on suppose $H(n)$, soit $E$ de dimension $n+1$ et $u\in\L(E)$ tel que $\Tr(u)=0$. S'il existe $\lambda\in\K$ tel que $u=\lambda id_{E}$, on a $\Tr(u)=(n+1)\lambda=0$ donc $\lambda=0$ donc $u=0$. 

        Sinon, il existe $e_{1}\neq0$ tel que $(e_{1},u(e_{1}))$ est libre (résultat classique, redémontré en remarque ci-dessous). On pose $e_{2}=u(e_{1})$ et on complète $(e_{1},e_{2})$ en une base de $E$: $(e_{1},e_{2},\dots,e_{n+1})=\mathcal{B}_{1}$. Alors $\mat_{\mathcal{B}_{1}}(u)$ est de la forme 
        \begin{equation}
            \begin{pmatrix}
                0 & 
                \begin{matrix}
                    \star & \dots & \dots & \star    
                \end{matrix}\\
                \begin{matrix}
                    1\\
                    0\\
                    \vdots\\
                    0
                \end{matrix}
                & \mbox{\Huge A'}
            \end{pmatrix}
        \end{equation}
        avec $\Tr(u)=\Tr(A')=0$. Posons $F=\Vect(e_{2},\dots,e_{n+1})$. On note $\Pi$ la projection sur $F$ parallèlement à $\Vect(e_{1})$. Alors si \function{u'}{F}{F}{x}{\Pi(u(x))}
        et $A'=\mat_{(e_{2},\dots,e_{n+1})}(u')$ donc $\Tr(u')=0$. D'après $H(n)$, il existe $(f_{2},\dots,f_{n+1})$ une base de $F$ telle que 
        \begin{equation}
            \mat_{(f_{2},\dots,f_{n+1})}(u')=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
        \end{equation}

        Soit donc $\mathcal{B}_{2}=(e_{1},f_{2},\dots,f_{n+1})$ base de $E$. On a $u(e_{1})\in F$ donc 
        \begin{equation}
            \boxed{
            \mat_{\mathcal{B}_{2}}(u)=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
            }
        \end{equation}

        \item Soit $M=(a_{i,j})_{1\leqslant i,j\leqslant n}$ et $D=(i\delta_{i,j})_{1\leqslant i,j\leqslant n}$. On a 
        \begin{equation}
            [DM]_{i,j}=\sum_{k=1}^{n}i\delta_{i,k}a_{k,j}=ia_{i,j}
        \end{equation}
        et 
        \begin{equation}
            [MD]_{i,j}=\sum_{k=1}^{n}a_{i,k}k\delta_{k,j}=ja_{i,j}
        \end{equation}

        On a $M\in\ker(\varphi)$ si et seulement si pour tout $i\neq j$, $a_{i,j}=0$ si et seulement si $M\in D_{n}(\K)$ (ensemble des matrices diagonales). Donc $\dim(\ker(\varphi))=n$ et $\dim(\im(\varphi))=n^{n}-n$. Or pour tout $M\in \M_{n}(\K), [MD-DM]_{i,i}=0$. Notons $\Delta_{n}$ l'ensemble des matrices de diagonale nulle. On a $\im\varphi\subset\Delta_{n}$ et $\dim(\Delta_{n})=n^{2}-n$ (une base de $\Delta_{n}$ est $(E_{i,j})_{i\neq j}$, matrices élémentaires) donc $\im(\varphi)=\Delta_{n}$.

        Soit alors $A\in\M_{n}(\K)$ telle que $\Tr(A)=0$. D'après 1. il existe $P\in GL_{n}(\K)$ telle que $P^{-1}AP\in\Delta_{n}=\im(\varphi)$ donc il existe $M\in\M_{n}(\K)$ telle que $P^{-1}AP=MD-DM$ donc 
        \begin{align}
            A
            &=P(MD-DM)P^{-1}\\
            &=PM DP^{-1}-PD MP^{-1}\\
            &=\boxed{XY-YX}
        \end{align}
        avec $X=PMP^{-1}$ et $Y=PDP^{-1}$.
    \end{enumerate}
\end{proof}

\begin{remark}
    Soit $E$ un $\K$-espace vectoriel et $u\in\L(E)$ tel que pour tout $x\in E\setminus\lbrace0\rbrace$, $(x,u(x))$ est liée i.e.~pour tout $x\in E\setminus\lbrace0\rbrace$, $u(x)=\lambda_{x}x$. Alors $u$ est une homothétie.

    En effet, soit $(x,y)\in\left(E\setminus\lbrace0\rbrace\right)^{2}$, si $(x,y)$ est liée, il existe $\mu\in\K^{*}$ tel que $y=\mu x$. On a alors 
    \begin{equation}
        u(y)=\lambda_{y}y=\mu u(x)=\mu\lambda_{x}x=\lambda_{x}y
    \end{equation}
    On a $y\neq0$ donc $\lambda_{x}=\lambda_{y}$.

    Si $(x,y)$ est libre, on a
    \begin{equation}
        u(x+y)=\lambda_{x+y}(x+y)=\lambda_{x}x+\lambda_{y}y
    \end{equation}
    Par liberté de $(x,y)$, on a $\lambda_{x+y}=\lambda_{x}=\lambda_{y}$. 
    
    Ainsi, $\lambda_{x}$ ne dépend pas de $x$: il existe $\lambda\in\K$ tel que pour tout $x\in E$, $u(x)=\lambda x$, i.e.~$u=\lambda id_{E}$.
\end{remark}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Si
        $X=\begin{pmatrix}
            x_{1} & \dots &x_{n}
        \end{pmatrix}^{\mathsf{T}}$ et $Y=\begin{pmatrix}
            x_{1} & \dots &x_{n}
        \end{pmatrix}^{\mathsf{T}}$, on a 
        \begin{equation}
            XY^{\mathsf{T}}=
            \begin{pmatrix}
                x_{1}y_{1}&\dots&\dots&x_{1}y_{n}\\
                \vdots & & & \vdots\\
                \vdots & & & \vdots\\
                x_{n}y_{1} & \dots & \dots & x_{n}y_{n}
            \end{pmatrix}
        \end{equation}
        est de rang 1. On a 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=X(Y^{\mathsf{T}}X)Y^{\mathsf{T}}=\left(\sum_{i=1}^{n}x_{i}y_{i}\right)XY^{\mathsf{T}}
        \end{equation}

        Si $\lambda=0$, c'est évident.

        Si $\lambda\neq0$ et $B=I_{n}+\lambda XY^{\mathsf{T}}$, on a
        \begin{equation}
            XY^{\mathsf{T}}=\frac{B-I_{n}}{\lambda}
        \end{equation}
        et 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=\frac{\left(B-I_{n}\right)^{2}}{\lambda^{2}}
        \end{equation}
        soit 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=\frac{B^{2}-2B+I_{n}}{\lambda^{2}}=\left(\sum_{i=1}^{n}x_{i}y_{i}\right)\left(\frac{B-I_{n}}{\lambda}\right)
        \end{equation}
        d'où 
        \begin{equation}
            \lambda\left(Y^{\mathsf{T}}X\right)\left(B-I_{n}\right)=B^{2}-2B+I_{n}
        \end{equation}
        d'où
        \begin{equation}
            B^{2}+\left(-2-\lambda\left(Y^{\mathsf{T}}X\right)\right)B+I_{n}\left(1+\lambda\left(Y^{\mathsf{T}}X\right)\right)=0
        \end{equation}

        Si $1+\lambda Y^{\mathsf{T}}X\neq0$, alors $B$ est inversible et 
        \begin{equation}
            \boxed{B^{-1}=-\frac{1}{1+\lambda Y^{\mathsf{T}}X}\left(B-\left(2+\lambda Y^{\mathsf{T}}X\right)I_{n}\right)}
        \end{equation}

        Si $1+\lambda Y^{\mathsf{T}}X=0$, on a 
        \begin{equation}
            B\left(B-I_{n}\right)=0
        \end{equation}
        Si $B$ est inversible, on aura $B=I_{n}$ et $\lambda XY^{\mathsf{T}}=O_{\M_{n}(\K)}$. Or $\lambda\neq0$ donc $X=Y=0$ et $1=0$: absurde. Donc $B\notin GL_{n}(\K)$.

        \item On a 
        \begin{equation}
            M=A+\lambda XY^{Y}=A\left(I_{n}+\lambda A^{-1}XY^{\mathsf{T}}\right)
        \end{equation}
        donc $M\in GL_{n}(\R)$ si et seulement si $\left(I_{n}+\lambda A^{-1}XY^{\mathsf{T}}\right)$ est inversible si et seulement si $1+\lambda Y^{\mathsf{T}}A^{-1}X$ est inversible d'après 1. Alors 
        \begin{equation}
            \boxed{M^{-1}=\left(I_{n}-\frac{\lambda A^{-1}XY^{\mathsf{T}}}{1+\lambda Y^{\mathsf{T}}A^{-1}X}\right)A^{-1}}
        \end{equation}
    \end{enumerate}
\end{proof}

\begin{proof}
    On a $\dim(\R_{n}[X])=n+1$ donc il faut montrer que $(S_{0},\dots, S_{n})$ est libre. Soit donc $\alpha=\left(\alpha_{0},\dots,\alpha_{n}\right)\in\R^{n+1}$ tel que 
    \begin{equation}
        \alpha_{0}S_{0}+\dots+\alpha_{n}S_{n}=0
    \end{equation}

    Si $\alpha\neq0$, on pose $k_{0}=\max\left(k\in\llbracket0,n\rrbracket\middle|\alpha_{k}\neq0\right)$. On a 
    \begin{equation}
        \alpha_{0}(1-X)^{n}+\dots+\alpha_{k_{0}}X^{k_{0}}(1-X)^{n-k_{0}}=0
    \end{equation}
    soit 
    \begin{equation}
        \alpha_{0}(1-X)^{k_{0}}+\dots+\alpha_{k_{0}}X^{k_{0}}=0
    \end{equation}
    En évaluant en 1, on a $\alpha_{k_{0}}=0$ ce qui est absurde. Donc $(S_{0},\dots,S_{n})$ est une base de $\R_{n}[X]=n+1$.

    Pour tout $k\in\llbracket0,n\rrbracket$, on a 
    \begin{align}
        S_{j}
        &=X^{j}(1-X)^{n-j}\\
        &=X^{j}\left(\sum_{k=0}^{n-j}\binom{n-j}{k}(-1)^{k}X^{k}\right)\\
        &=\sum_{k=0}^{n-j}\binom{n-j}{k}(-1)^{k}X^{k+j}\\
        &=\sum_{k=j}^{n}\binom{n-j}{k-j}(-1)^{k-j}X^{k}
    \end{align}
    donc 
    \begin{equation}
        \boxed{
            A=P_{(1,\dots,X^{n})\to(S_{0},\dots, S_{n})}=\left(\binom{n-j}{k-j}(-1)^{k-j}\right)_{0\leqslant k,j\leqslant n}
        }
    \end{equation}

    On considère $u\in\L\left(\R[X]\right)$ tel que $u(X^{j})=S_{j}$ pour tout $j\in\llbracket0,n\rrbracket$. On a $u(X^{j})=\left(\frac{X}{1-X}\right)^{j}(1-X)^{n}$. Pour tout $P\in\R_{n}[X]$, on a $u(P)=P\left(\frac{X}{1-X}\right)(1-X)^{n}$. Soit $(P,Q)\in\R_{n}[X]^{2}$, on a $u(P)=Q$ si et seulement si $P\left(\frac{X}{1-X}\right)(1-X)^{n}=Q(X)$ si et seulement si $P(Y)\left(\frac{1}{1+Y}\right)^{n}=Q\left(\frac{Y}{1+Y}\right)$ soit $u(P)=Q$ si et seulement si $P(Y)=Q\left(\frac{Y}{1+Y}\right)(1+Y)^{n}$. Ainsi $u^{-1}(X^{j})=X^{j}(1+X)^{n-j}$, donc 
    \begin{equation}
        \boxed{
            A^{-1}=\mat_{(1,\dots,X^{n})}(u^{-1})=\left(\binom{n-j}{k-j}\right)_{0\leqslant k,j\leqslant n}
        }
    \end{equation}
\end{proof}

\begin{proof}
    Si on a $H\cap GL_{n}(\K)=\emptyset$, on a $I_{n}\notin H$. On écrit donc 
    \begin{equation}
        \M_{n}(\K)=H\oplus \K I_{n}
    \end{equation}
    Soit $i\neq j$, on prend $E_{i,j}=M+\lambda I_{n}$ (décomposition précédente) avec $\lambda\in\K$. Si $\lambda\neq0$, on a
    \begin{equation}
        M=E_{i,j}-\lambda I_{n}\in GL_{n}(\K)
    \end{equation}
    donc $M\in GL_{n}(\K)\cap H$: absurde. Donc $\lambda=0$ et $E_{i,j}\in H$, d'où $\Vect(E_{i,j})_{,\neq i}\subset H$. Or
    \begin{equation}
        \begin{pmatrix}
            0 & \dots & \dots & 0 & 1\\
            1 & \ddots & & & 0\\
            0 & \ddots & \ddots& & \vdots\\
            \vdots & \ddots & \ddots&\ddots & \vdots\\
            0 & \dots& 0 & 1 & 0
        \end{pmatrix}\in \left(GL_{n}(\K)\cap\Vect(E_{i,j})_{i\neq j}\right)\subset \left(GL_{n}(\K)\cap H\right)
    \end{equation}
    donc $H\cap GL_{n}(\K)\neq\emptyset$: absurde.
\end{proof}

\begin{remark}
    Il existe une forme linéaire non nulle $\varphi\colon\M_{n}(\K)\to \K$ telle que $H=\ker(\varphi)$. 

    En effet, pour toute forme linéaire $\varphi$ sur $\M_{n}(\K)$, il existe une unique matrice $A\in\M_{n}(\K)$ telle que 
    \begin{equation}
        \varphi(M)=\Tr(AM)
    \end{equation}

    Pour le montrer: si $A$ existe, pour tout $(i,j)\in\llbracket1,n\rrbracket^{2}$, $\varphi(E_{i,j})=\Tr(AE_{i,j})=a_{j,i}$. Réciproquement, soit $A=\left(\varphi(E_{j,i})\right)_{1\leqslant i,j\leqslant n}$. On a pour tout $M\in\M_{n}(\K)$, $\varphi(M)=\Tr(AM)$ car cex deux formes linéaires coïncident sur les $(E_{i,j})_{1\leqslant i,j\leqslant n}$.

    Il existe donc $A\in \M_{n}(\K)\setminus\lbrace0\rbrace$,
    \begin{equation}
        H=\left\lbrace M\in\M_{n}(\K)\middle|\Tr(AM)=0\right\rbrace
    \end{equation}

    Si $r=\rg(A)$, il existe $(P,Q)\in GL_{n}(\K)^{2}$ telles que $A=Q^{-1}J_{n,n,r}P$ ($J_{n,n,r}$: matrice de taille $n\times n$ avec les $r$ premiers coefficients diagonaux valant 1). Alors pour tout $M\in \M_{n}(\K)$, on a 
    \begin{equation}
        \Tr(AM)=\Tr(J_{n,n,r}\underbrace{MPQ^{-1}}_{=~M'})
    \end{equation}
    et il suffit de prendre 
    \begin{equation}
        M'=
        \begin{pmatrix}
            0 & \dots & \dots & 0 & 1\\
            1 & \ddots & & & 0\\
            0 & \ddots & \ddots& & \vdots\\
            \vdots & \ddots & \ddots&\ddots & \vdots\\
            0 & \dots& 0 & 1 & 0
        \end{pmatrix}
    \end{equation}
\end{remark}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item On prend $\lambda=0$ et $N(0\times 0)=0\times N(0)=0$ donc 
        \begin{equation}
            \boxed{N(0)=0}
        \end{equation}

        \item On a pour $j\neq i,$ $E_{i,j}\times E_{j,j}=E_{i,j}$ et $E_{j,j}E_{i,j}=0$ donc $N(E_{i,j})=N(E_{i,j}E_{j,j})=N(E_{j,j}E_{i,j})$ d'où 
        \begin{equation}
            \boxed{N(E_{i,j})=0}
        \end{equation}

        \item Déjà traité à l'exercice~\ref{sol:4.4}.
        
        \item Si $\Tr(A)=0$, alors il existe $P\in GL_{n}(\C)$ telle que 
        \begin{equation}
            P^{-1}AP=\sum_{i\neq j}\alpha_{i,j}E_{i,j}
        \end{equation}
        donc 
        \begin{equation}
            \boxed{N(A)=N(P^{-1}AP)\leqslant\sum_{i\neq j}\alpha_{i,j}N(E_{i,j})=0}
        \end{equation}

        \item Soit $A'=A-\frac{\Tr(A)}{n}I_{n}$. On a $N(A')=0$ d'après ce qui précède. Montrons que pour tout $(A,B)\in\M_{n}(\C)^{2}$,
        \begin{equation}
            \left\lvert N(A)-N(B)\right\rvert\leqslant N(A-B)
        \end{equation}
        On écrit $A=A-B+B$ et $N(A)\leqslant N(A-B)+N(B)$ d'où $N(A)-N(B)\leqslant N(A-B)$ et on a le résultat par symétrie de $A$ et $B$.

        On a donc 
        \begin{equation}
            \left\lvert N(A)-N\left(\frac{\Tr(A)}{n}I_{n}\right)\right\rvert\leqslant N\left(A-\frac{\Tr(A)}{n}I_{n}\right)=0
        \end{equation}
        d'où 
        \begin{equation}
            \boxed{N(A)=N\left(\frac{\Tr(A)}{n}I_{n}\right)=\left\lvert\Tr(A)\right\rvert\times \underbrace{N\left(\frac{I_{n}}{n}\right)}_{=~a\geqslant0}}
        \end{equation}
    \end{enumerate}
\end{proof}

\begin{proof}
    On écrit 
    \begin{equation}
        f+g=f\circ\left(id+f^{-1}\circ g\right)
    \end{equation}
    avec $f^{-1}\circ g$ de rang 1. Il existe une base $\mathcal{B}
    $ de $E$ telle que 
    \begin{equation}
        \mat_{\mathcal{B}}(f^{-1}\circ g)=
        \begin{pmatrix}
            0 & \dots & \dots & 0 & \star\\
            \vdots & & &\vdots &\vdots\\
            \vdots & & &\vdots & \vdots\\
            0 & \dots & \dots & 0 & \alpha
        \end{pmatrix}
    \end{equation}
    avec $\alpha=\Tr(f^{-1}\circ g)$ et donc $\mat_{\mathcal{B}}(id+g^{-1}\circ g)$ est inversible si et seulement si $1+\alpha\neq0$ si et seulement si $\Tr(f^{-1}\circ g)\neq 1$.
\end{proof}

\begin{proof}
    Par symétrie du problème, il suffit de déterminer les
    \begin{equation}
        a_{n,j}=\left\lvert\left\lbrace\text{chemins de longueur }n\text{ de }1\text{ vers }j\in\left\lbrace2,3,4\right\rbrace\right\rbrace\right\rvert
    \end{equation}

    On pose 
    \begin{equation}
        X_{n}=
        \begin{pmatrix}
            a(n,1)\\
            a(n,2)\\
            a(n,3)\\
            a(n,4)
        \end{pmatrix}
    \end{equation}
    On a alors 
    \begin{equation}
        X_{n+1}=
        \underbrace{
        \begin{pmatrix}
            0 &1 &0 &1\\
            1 &0 &1 &0\\
            0 &1 &0 &1\\
            1 &0 &1 &0
        \end{pmatrix}}_{=~A}
        X_{n}
    \end{equation}
    car (en raisonnant modulo 4) il y a autant de chemins de longueur n+1 reliant 1 à j que de chemins de longueur n reliant 1 à j-1 + chemins de longueur n reliant 1 à j+1.
    d'où $X_{n}=A^{n}X_{0}$ avec 
    \begin{equation}
        X_{0}=
        \begin{pmatrix}
            1\\0\\0\\0
        \end{pmatrix}
    \end{equation}
    On a
    \begin{equation}
        A=
        \begin{pmatrix}
            B&B\\B&B
        \end{pmatrix}
    \end{equation}
    avec
    \begin{equation}
        B=
        \begin{pmatrix}
            0&1\\1&0
        \end{pmatrix}
    \end{equation}
    On a $B^{2}=I_{2}$ et on montre par récurrence
    \begin{equation}
        \left\lbrace
            \begin{array}[]{ll}
                A^{2p}=2^{2p-1}
                \begin{pmatrix}
                    I_{2} &I_{2}\\ I_{2}&I_{2}
                \end{pmatrix} &p\geqslant1\\
                A^{2p+1}=2^{2p}
                \begin{pmatrix}
                    B & B\\B &B
                \end{pmatrix}&p\geqslant0
            \end{array}
        \right.
    \end{equation}

    Ainsi,
    \begin{equation}
        \boxed{
            \begin{array}[]{l}
                a(2p,1)=2^{2p-1}=a(2p,3)\\
                a(2p,2)=0=a(2p,4)\\
                a(2p+1,1)=0=a(2p+1,3)\\
                a(2p+1,4)=2^{2p}=a(2p+1,4)\\
            \end{array}
        }
    \end{equation}

    \item Ici, on a 
    \begin{equation}
        A=
        \begin{pmatrix}
            0 &1 &0 &1 &0 &1 &0 &0\\
            1 &0 &1 &0 &0 &0 &1 &0\\
            0 &1 &0 &1 &0 &0 &0 &1\\
            1 &0 &1 &0 &1 &0 &0 &0\\
            0 &0 &0 &1 &0 &1 &0 &1\\
            1 &0 &0 &0 &1 &0 &1 &0\\
            0 &1 &0 &0 &0 &1 &0 &1\\
            0 &0 &1 &0 &1 &0 &1 &0
        \end{pmatrix}
    \end{equation}

    En deux itérations, il y a chaque fois deux possibilités pour relier deux sommets différents de même partié, et 3 pour revenir au même sommet. On a donc 
    \begin{equation}
        A^{2}=
        \begin{pmatrix}
            3 &0 &2 &0 &2 &0 &2 &0\\
            0 &3 &0 &2 &0 &2 &0 &2\\
            2 &0 &3 &0 &2 &0 &2 &0\\
            0 &2 &0 &3 &0 &2 &0 &2\\
            2 &0 &2 &0 &3 &0 &2 &0\\
            0 &2 &0 &2 &0 &3 &0 &2\\
            2 &0 &2 &0 &2 &0 &3 &0\\
            0 &2 &0 &2 &0 &2 &0 &3
        \end{pmatrix}
        =I_{8}+2
        \begin{pmatrix}
            B &B &B &B\\
            B &B &B &B\\
            B &B &B &B\\
            B &B &B &B
        \end{pmatrix}
    \end{equation}

    On applique le binôme de Newton pour calculer les puissances paires de $A$, puis on déduit les puissances impaires en multipliant par $A$.
\end{proof}

\begin{proof}
    Soit $X=\begin{pmatrix}
        x_{1}&\dots&x_{n}
    \end{pmatrix}^{\mathsf{T}}\in\M_{n,1}(\C)$. Supposons $AX=0$. Alors pour tout $i\in\llbracket1,n\rrbracket$,
    \begin{equation}
        \sum_{j=1}^{n}a_{i,j}x_{j}=0\Rightarrow -a_{i,i}x_{i}=\sum_{\substack{j=1\\j\neq i}}^{n}a_{i,j}x_{j}
    \end{equation}
    donc 
    \begin{equation}
        \left\lvert\sum_{\substack{j=1\\j\neq i}}^{n}a_{i,j}x_{j}\right\rvert=\left\lvert a_{i,i}x_{i}\right\rvert\leqslant\sum_{\substack{j=1\\j\neq i}}^{n}\left\lvert a_{i,j}x_{j}\right\rvert
    \end{equation}
    Soit $i_{0}\in\llbracket1,n\rrbracket$ tel que 
    \begin{equation}
        x_{i_{0}}=\max\left\lbrace\left\lvert x_{i}\right\rvert,i\in\llbracket1,n\rrbracket\right\rbrace
    \end{equation}

    On a alors 
    \begin{equation}
        \left\lvert a_{j,i_{0}}\right\rvert\left\lvert x_{i_{0}}\right\rvert\leqslant \left\lvert x_{i_{0}}\right\rvert\sum_{\substack{j=1\\j\neq i}}^{n}\left\lvert a_{i_{0},j}\right\rvert
    \end{equation}
    D'après l'hypothèse, on a $\left\lvert x_{i_{0}}\right\rvert=0$ donc $X=0$ et $A$ est inversible.

    Il faut l'inégalité stricte, un contre-exemple est donnée par une ligne nulle.
\end{proof}

\begin{remark}
    Si pour tout $j\in\llbracket1,n\rrbracket$, $\left\lvert a_{j,j}\right\rvert>\sum_{i\neq j}\left\lvert a_{i,j}\right\rvert$ alors $A^{\mathsf{T}}\in GL_{n}(\C)$ et donc $A\in GL_{n}(\C)$.
\end{remark}

\begin{proof}
    On écrit, pour tout $(i,j)\in\llbracket1,n\rrbracket^{2}$,
    \begin{align}
        i\wedge j
        &=\sum_{k\mid i\wedge j}\varphi(k)\\
        &=\sum_{\substack{k\mid i\\ k\mid j}}\varphi(k)\\
        &=\sum_{k=1}^{n}b_{k,i}b_{k,j}\varphi(k)
    \end{align}
    avec $b_{k,i}=1$ si $k\mid i$ et 0 sinon. On a alors, si $A=\left(i\wedge j\right)_{1\leqslant i,j\leqslant n}$, $A=B^{\mathsf{T}}C$ avec $B=(b_{k,i})_{1\leqslant i,k\leqslant n}$ (triangulaire supérieure) et $C=\left(\varphi(k)b_{k,j}\right)_{1\leqslant k,j\leqslant n}$ (triangulaire supérieure). Donc 
    \begin{equation}
        \boxed{\det(A)=\prod_{i=1}^{n}\varphi(i)}
    \end{equation}
\end{proof}

\begin{proof}
    Pour l'unicité, si $A=L_{1}U_{1}=L_{2}U_{2}$ telles que proposées. Comme $A$ est inversible, on a $\det(A)=\det(L_{i})\det(U_{i})\neq0$ pour $i\lbrace1,2\rbrace$ et donc $L_{i}$ et $U_{i}$ sont inversibles. Ainsi,
    \begin{equation}
        L_{2}^{-1}L_{1}=U_{2}U_{1}^{-1}\in \mathcal{T}_{n}^{-}(\C)\cap\mathcal{T}_{n}^{+}(\C)
    \end{equation}
    avec des 1 sur la diagonale, c'est donc $I_{n}$, d'où l'unicité.

    Pour l'existence, on travaille par récurrence sur $n\in\N$: pour $n=1$ on a $A=(1)\times(a_{1,1})$. Soit $A_{n+1}\in \M_{n+1}(\C)$ vérifiant l'hypothèse, alors $A_{n}$ vérifie l'hypothèse $A_{n}=L_{n}U_{n}$ avec 
    \begin{equation}
        A_{n+1}=
        \begin{pmatrix}
            A_{n} & Y\\
            X^{\mathsf{T}} & a_{n+1,n+1}
        \end{pmatrix}
    \end{equation}

    On veut 
    \begin{equation}
        A_{n+1}=
        \begin{pmatrix}
            L' &\begin{matrix}
                0\\\vdots\\0
            \end{matrix}\\
            X_{1}^{\mathsf{T}} &1
        \end{pmatrix}\times\begin{pmatrix}
            U' & Y_{1}\\
            \begin{matrix}
                0&\dots&0
            \end{matrix}&u_{n+1,n+1}
        \end{pmatrix}
    \end{equation}
    On a $(X,Y)\in\M_{n+1}(\C)$, par produits par blocs, on a $A_{n}=L'U'=L_{n}U_{n}$ et par unicité, $L'=L_{n}$ et $U'=U_{n}$. On a $X^{\mathsf{T}}=X_{1}^{\mathsf{T}}U'$ et donc $X_{1}^{\mathsf{T}}=X^{\mathsf{T}}U_{n}^{-1}$ et $Y=L_{n}Y_{1}$ donc $Y_{1}=L_{n}^{-1}Y$.

    Enfin, $a_{n+1,n+1}=X_{1}^{\mathsf{T}}Y_{1}+u_{n+1,n+1}$ et donc 
    \begin{equation}
        u_{n+1,n+1}=a_{n+1,n+1}-X_{1}^{\mathsf{T}}Y_{1}=a_{n+1,n+1}-X^{\mathsf{T}}U_{n}^{-1}L_{n}^{-1}Y
    \end{equation}

    Réciproquement, en définissant ainsi $U$ et $L$, on a bien $A=Lu$ en remontant les calculs.
\end{proof}

\begin{proof}
    On a $\sum_{k\in A_{i}}a_{k}-\sum_{k\in B_{i}}a_{k}=0$ (combinaison linéaire des $a_{k}$ avec des coefficients $\pm1$), donc 
    \begin{equation}
        \underbrace{
            \begin{pmatrix}
                0 & \pm 1&\dots&\dots&\pm1\\
                \pm 1&\ddots&\ddots&&\vdots\\
                \vdots&\ddots&\ddots&\ddots&\vdots\\
                \vdots&&\ddots&\ddots&\pm1\\
                \pm1&\dots&\dots&\pm1&0
            \end{pmatrix}
        }_{=~A}
        \underbrace{
            \begin{pmatrix}
                a_{1}\\
                \vdots\\
                a_{2n+1}
            \end{pmatrix}
        }_{=~X}=0
    \end{equation}

    Sur chaque ligne, il y a $n$ fois 1 et $n$ fois -1 (car les $A_{i}$ et $B_{i}$ sont disjoints). On veut montrer que $X=\alpha\bm{1}$. On a $X\in\ker(A)$ et $\bm{1}\in\ker(A)$ (car il y a $n$ 1 et $n$ -1 par ligne). On veut donc montrer que $\dim(\ker(A))=1$, soit $\rg(A)=2n$.

    On doit donc montrer qu'il existe une sous-matrice de taille $2n$ inversible car $\dim(\ker(A))\geqslant1$. Comme on est bloqué par les $\pm1$, on se place dans $\Z/2\Z$. Soit donc 
    \begin{equation}
        \overline{B_{n}}=
        \begin{pmatrix}
            \overline{0} & \overline{1}&\dots&\dots&\overline{1}1\\
            \overline{1}&\ddots&\ddots&&\vdots\\
            \vdots&\ddots&\ddots&\ddots&\vdots\\
            \vdots&&\ddots&\ddots&\overline{1}1\\
            \overline{1}1&\dots&\dots&\overline{1}1&\overline{0}
        \end{pmatrix}\in\M_{n}\left(\Z/2\Z\right)
    \end{equation}
    Si $\det(\overline{B_{n}})\neq0$, on a $\det(B_{n})\neq 2k$ pour tout $k\in\N$ où $B_{n}$ est obtenue en enlevant à $A$ sa dernière ligne et sa dernière colonne, et donc $\det(A)\neq0$.

    On cherche un polynôme annulateur de $\overline{B_{n}}$. On a 
    \begin{equation}
        \left(\overline{B_{n}}+\overline{I_{2n}}\right)^{2}=\overline{B_{n}}^{2}+2\overline{B_{n}}+I_{2n}=\begin{pmatrix}
            \overline{1}&\dots&\overline{1}\\
            \vdots & & \vdots\\
            \overline{1}&\dots&\overline{1}
        \end{pmatrix}^{2}=2n
        \begin{pmatrix}
            \overline{1}&\dots&\overline{1}\\
            \vdots & & \vdots\\
            \overline{1}&\dots&\overline{1}
        \end{pmatrix}=\begin{pmatrix}
            \overline{0}
        \end{pmatrix}
    \end{equation}

    Ainsi,
    \begin{equation}
        \overline{B_{n}}\left(\overline{B_{n}}+2\overline{I_{2n}}\right)=-\overline{I_{2n}}=\overline{I_{2n}}
    \end{equation}
    donc $\overline{B_{n}}\in GL_{n}\left(\Z/2\Z\right)$ et donc $B_{n}\in GL_{2n}(\R)$, ce qui démontre bien que $\rg(A)=2n$ et $\ker(A)=\Vect(\bm{1})$, d'où
    \begin{equation}
        \boxed{a_{1}=\dots=a_{2n+1}}
    \end{equation}
\end{proof}